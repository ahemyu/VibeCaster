{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# 'C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Software_5.json'\n",
    "#C:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\data\\Software_5.json(1).gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the datset into DataFrame as described on the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/#subsets\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "# small dataset, ca. 77 000 Entries\n",
    "# df = getDF('C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Industrial_and_Scientific_5.json.gz')\n",
    "# way bigger dataset ca. 500 000 Entries\n",
    "df = getDF('C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Video_Games_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497577, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 497577 entries, 0 to 497576\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   overall         497577 non-null  float64\n",
      " 1   verified        497577 non-null  bool   \n",
      " 2   reviewTime      497577 non-null  object \n",
      " 3   reviewerID      497577 non-null  object \n",
      " 4   asin            497577 non-null  object \n",
      " 5   reviewerName    497501 non-null  object \n",
      " 6   reviewText      497419 non-null  object \n",
      " 7   summary         497468 non-null  object \n",
      " 8   unixReviewTime  497577 non-null  int64  \n",
      " 9   vote            107793 non-null  object \n",
      " 10  style           289237 non-null  object \n",
      " 11  image           3634 non-null    object \n",
      "dtypes: bool(1), float64(1), int64(1), object(9)\n",
      "memory usage: 46.0+ MB\n",
      "None\n",
      "             overall  unixReviewTime\n",
      "count  497577.000000    4.975770e+05\n",
      "mean        4.220456    1.367848e+09\n",
      "std         1.185424    1.224113e+08\n",
      "min         1.000000    9.398592e+08\n",
      "25%         4.000000    1.316563e+09\n",
      "50%         5.000000    1.410221e+09\n",
      "75%         5.000000    1.452384e+09\n",
      "max         5.000000    1.538438e+09\n"
     ]
    }
   ],
   "source": [
    " # summary statistics\n",
    "print(df.shape) # \n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall                0\n",
      "verified               0\n",
      "reviewTime             0\n",
      "reviewerID             0\n",
      "asin                   0\n",
      "reviewerName          76\n",
      "reviewText           158\n",
      "summary              109\n",
      "unixReviewTime         0\n",
      "vote              389784\n",
      "style             208340\n",
      "image             493943\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete entrys with missing reviewText: \n",
    "df.dropna(subset=['reviewText'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep overall and reviewText\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "columns_to_keep = ['overall', 'reviewText']\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             overall\n",
      "count  497419.000000\n",
      "mean        4.220297\n",
      "std         1.185491\n",
      "min         1.000000\n",
      "25%         4.000000\n",
      "50%         5.000000\n",
      "75%         5.000000\n",
      "max         5.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col sentiment to train the model on\n",
    "\n",
    "def classify_sentiment(overall_score):\n",
    "    if 1 <= overall_score <= 2:\n",
    "        return 1  # Negative\n",
    "    elif overall_score == 3:\n",
    "        return 0  # Neutral\n",
    "    elif 4 <= overall_score <= 5:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Apply the function to the 'overall' column to create the 'sentiment' column\n",
    "df['sentiment'] = df['overall'].apply(classify_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText  sentiment\n",
      "0      5.0  This game is a bit hard to get the hang of, bu...          2\n",
      "1      4.0  I played it a while but it was alright. The st...          2\n",
      "2      3.0                                           ok game.          0\n",
      "3      2.0  found the game a bit too complicated, not what...          1\n",
      "4      5.0  great game, I love it and have played it since...          2\n",
      "sentiment\n",
      "2    393267\n",
      "1     55012\n",
      "0     49140\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "# Count the number of each sentiment\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset is heavily skewed on the positive side of things so might need to use cross-validation to account for it\n",
    "# we only need reviewText and Sentiment to start training our model so lets drop the 'overall' column\n",
    "df.drop(columns=\"overall\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Preprocessing Steps:\n",
    "\n",
    "#     Text Cleaning:\n",
    "#         Why: Raw text often contains punctuations, numbers, and special characters that don't contribute much to the sentiment.\n",
    "#         How: Regular expressions or string manipulation techniques can be used for this.\n",
    "\n",
    "#     Tokenization:\n",
    "#         Why: Tokenization helps to break down the text into smaller pieces, often into words, which makes it easier for the algorithm to identify patterns.\n",
    "#         How: Libraries like NLTK and spaCy provide tokenization methods.\n",
    "\n",
    "#     Stemming/Lemmatization:\n",
    "#         Why: Different forms of a word often convey the same sentiment (e.g., 'running' and 'ran'). Stemming and Lemmatization convert words to their base or root form.\n",
    "#         How: Again, NLTK and spaCy have methods for these.\n",
    "\n",
    "#     Removal of Stop Words:\n",
    "#         Why: Commonly occurring words (like 'and', 'the', 'is') generally don't contribute to the sentiment and can be removed.\n",
    "#         How: Predefined lists of stop words are available in NLTK and spaCy.\n",
    "\n",
    "#     Feature Extraction:\n",
    "#         Why: Machine learning algorithms require numerical input, and the text needs to be converted into a format like Bag-of-Words or TF-IDF that can be fed into these algorithms.\n",
    "#         How: Scikit-learn provides CountVectorizer for Bag-of-Words and TfidfVectorizer for TF-IDF.\n",
    "\n",
    "#     Handling Class Imbalance:\n",
    "#         Why: Given that your dataset is imbalanced, using techniques to either oversample the minority class or undersample the majority class can make the model more fair.\n",
    "#         How: Libraries like imblearn provide methods like SMOTE for oversampling.\n",
    "\n",
    "\n",
    "\n",
    "#### Data Preprocessing####\n",
    "## Step1: Text Cleaning ## \n",
    "# Remove punctuations\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub('[^\\w\\s]', '', x))\n",
    "\n",
    "## Step 2: Tokenization ##\n",
    "df['tokenized_reviewText'] = df['reviewText'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# ## Step3: Lemmatization ## \n",
    "# from nltk.corpus import wordnet\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# def get_wordnet_pos(tag):\n",
    "#     \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "#     tag = tag[0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# # Function to apply lemmatization to a list of words with POS tagging\n",
    "# def lemmatize_with_pos(words):\n",
    "#     pos_tagged = nltk.pos_tag(words)\n",
    "#     return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged]\n",
    "\n",
    "# # Apply POS tagging and lemmatization\n",
    "# df['lemmatized_reviewText'] = df['tokenized_reviewText'].apply(lemmatize_with_pos)\n",
    "\n",
    "## Step3: Lemmatization ##\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer  # Import the WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to apply lemmatization to a list of words with POS tagging\n",
    "def lemmatize_with_pos(words):\n",
    "    pos_tagged = nltk.pos_tag(words)\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged]\n",
    "\n",
    "# Apply POS tagging and lemmatization\n",
    "df['lemmatized_reviewText'] = df['tokenized_reviewText'].apply(lemmatize_with_pos)\n",
    "\n",
    "## use concurrency to speed up lemmatization ## \n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize the WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def get_wordnet_pos(tag):\n",
    "#     \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "#     tag = tag[0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# def lemmatize_with_pos(words):\n",
    "#     pos_tagged = nltk.pos_tag(words)\n",
    "#     return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged]\n",
    "\n",
    "# def parallel_lemmatization(df_split):\n",
    "#     return df_split['tokenized_reviewText'].apply(lemmatize_with_pos)\n",
    "\n",
    "# # Split the DataFrame into smaller parts for parallel processing\n",
    "# num_splits = 8  # Number of splits\n",
    "# df_splits = np.array_split(df, num_splits)\n",
    "\n",
    "# # Use ProcessPoolExecutor to parallelize the function\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "#     df_splits_lemmatized = list(executor.map(parallel_lemmatization, df_splits))\n",
    "\n",
    "# # Concatenate the splits back into a single DataFrame\n",
    "# df_lemmatized = pd.concat(df_splits_lemmatized)\n",
    "# df['lemmatized_reviewText'] = df_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Step4: Removal of Stop Words to reduce dimensionality and complexity ##\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "df['no_stopwords_reviewText'] = df['lemmatized_reviewText'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up the DataFrame \n",
    "df = df[['no_stopwords_reviewText', 'sentiment']]\n",
    "df.rename(columns={'no_stopwords_reviewText': 'reviewText'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step5: Feature Extraction ##\n",
    "# Using Term Frequency-Inverse Document Frequency (TF-IDF) to get numerical representation of the text data# \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['reviewText'] = df['reviewText'].apply(' '.join)\n",
    "X = vectorizer.fit_transform(df['reviewText'])\n",
    "# Now, X is a sparse matrix containing the TF-IDF features\n",
    "\n",
    "import joblib\n",
    "# Save the TF-IDF vectorizer to disk\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step6: Handling Class Imbalance##\n",
    "# Using Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic data of minority class instances (neutral and negative sentiments) to mitigate class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# Fit on data\n",
    "X_resampled, y_resampled = smote.fit_resample(X, df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing model and trying it out ##\n",
    "\n",
    "# First lets try using Naive Bayes # \n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Splitting the dataset into Training set and Test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# # Initialize the Multinomial Naive Bayes classifier\n",
    "# nb_classifier = MultinomialNB()\n",
    "\n",
    "# # Fit the model\n",
    "# nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# # Calculating various metrics\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparam tuning: \n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "\n",
    "# # Initialize a Multinomial Naive Bayes classifier\n",
    "# nb_classifier = MultinomialNB()\n",
    "\n",
    "# # Initialize the GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, \n",
    "#                            cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# # Fit data to GridSearch\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Getting the best parameters\n",
    "# best_parameters = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f'Best Parameters: {best_parameters}')\n",
    "# print(f'Best Score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retraining the model with found hyperparam: \n",
    "# # Initialize the Multinomial Naive Bayes classifier with the best parameter\n",
    "# nb_classifier = MultinomialNB(alpha=0.001)\n",
    "\n",
    "# nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7865367581930912\n",
      "Precision: 0.7866504314855324\n",
      "Recall: 0.7865367581930912\n",
      "F1 Score: 0.7864313465960591\n"
     ]
    }
   ],
   "source": [
    "# Performance gain of approximately 5% after Fine-Tuning\n",
    "\n",
    "## Now lets try using Logistic regression to see if performance will increase (keeping in mind that overfitting is a real possibility)  ##\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(solver='saga', max_iter=10000) # saga = Stochastic Average Gradient Descent Algorithm \n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\Model_Training.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster/VibeCaster/Model_Training.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mlog_reg, param_grid\u001b[39m=\u001b[39mparam_grid, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster/VibeCaster/Model_Training.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                            cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster/VibeCaster/Model_Training.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Fit data to GridSearch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster/VibeCaster/Model_Training.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster/VibeCaster/Model_Training.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Get the best parameters and score\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster/VibeCaster/Model_Training.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m best_parameters \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\venv\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Hyperparameter Finetuning to (maybe) get better results ##\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit data to GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_parameters}')\n",
    "print(f'Best Score: {best_score}')\n",
    "\n",
    "# Retrain the model with best parameters\n",
    "log_reg_best = LogisticRegression(C=best_parameters['C'], penalty=best_parameters['penalty'], solver='saga')\n",
    "log_reg_best.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9108037437990274\n",
      "Precision: 0.9123433504215814\n",
      "Recall: 0.9108037437990274\n",
      "F1 Score: 0.9101739200299686\n"
     ]
    }
   ],
   "source": [
    "# C of 100 means High variance, low bias\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine_tuned_sentiment_model.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Performance gain of ≈ 7% after Fine Tuning\n",
    "# export the trained model to use in web interface\n",
    "import joblib\n",
    "\n",
    "# Save the fine-tuned model to disk\n",
    "joblib.dump(log_reg_best, 'fine_tuned_sentiment_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
