{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# 'C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Software_5.json'\n",
    "#C:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\data\\Software_5.json(1).gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the datset into DataFrame as described on the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/#subsets\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Industrial_and_Scientific_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77071, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77071 entries, 0 to 77070\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   overall         77071 non-null  float64\n",
      " 1   verified        77071 non-null  bool   \n",
      " 2   reviewTime      77071 non-null  object \n",
      " 3   reviewerID      77071 non-null  object \n",
      " 4   asin            77071 non-null  object \n",
      " 5   style           36037 non-null  object \n",
      " 6   reviewerName    77044 non-null  object \n",
      " 7   reviewText      77060 non-null  object \n",
      " 8   summary         77061 non-null  object \n",
      " 9   unixReviewTime  77071 non-null  int64  \n",
      " 10  vote            9620 non-null   object \n",
      " 11  image           1719 non-null   object \n",
      "dtypes: bool(1), float64(1), int64(1), object(9)\n",
      "memory usage: 7.1+ MB\n",
      "None\n",
      "            overall  unixReviewTime\n",
      "count  77071.000000    7.707100e+04\n",
      "mean       4.524062    1.454857e+09\n",
      "std        0.949668    4.559407e+07\n",
      "min        1.000000    1.051402e+09\n",
      "25%        4.000000    1.427674e+09\n",
      "50%        5.000000    1.459469e+09\n",
      "75%        5.000000    1.486944e+09\n",
      "max        5.000000    1.538093e+09\n"
     ]
    }
   ],
   "source": [
    " # summary statistics\n",
    "print(df.shape) # \n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall               0\n",
      "verified              0\n",
      "reviewTime            0\n",
      "reviewerID            0\n",
      "asin                  0\n",
      "style             41034\n",
      "reviewerName         27\n",
      "reviewText           11\n",
      "summary              10\n",
      "unixReviewTime        0\n",
      "vote              67451\n",
      "image             75352\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete entrys with missing reviewText: \n",
    "df.dropna(subset=['reviewText'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep overall and reviewText\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "columns_to_keep = ['overall', 'reviewText']\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            overall\n",
      "count  77060.000000\n",
      "mean       4.524046\n",
      "std        0.949636\n",
      "min        1.000000\n",
      "25%        4.000000\n",
      "50%        5.000000\n",
      "75%        5.000000\n",
      "max        5.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col sentiment to train the model on\n",
    "\n",
    "def classify_sentiment(overall_score):\n",
    "    if 1 <= overall_score <= 2:\n",
    "        return 1  # Negative\n",
    "    elif overall_score == 3:\n",
    "        return 0  # Neutral\n",
    "    elif 4 <= overall_score <= 5:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Apply the function to the 'overall' column to create the 'sentiment' column\n",
    "df['sentiment'] = df['overall'].apply(classify_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText  sentiment\n",
      "0      5.0  This worked really well for what I used it for...          2\n",
      "1      5.0                   Fast cutting and good adheasive.          2\n",
      "2      5.0  Worked great for my lapping bench.  I would li...          2\n",
      "3      4.0                                      As advertised          2\n",
      "4      5.0  seems like a pretty good value as opposed to b...          2\n",
      "sentiment\n",
      "2    68201\n",
      "0     4442\n",
      "1     4417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "# Count the number of each sentiment\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset is heavily skewed on the positive side of things so might need to use cross-validation to account for it\n",
    "# we only need reviewText and Sentiment to start training our model so lets drop the 'overall' column\n",
    "df.drop(columns=\"overall\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Preprocessing Steps:\n",
    "\n",
    "#     Text Cleaning:\n",
    "#         Why: Raw text often contains punctuations, numbers, and special characters that don't contribute much to the sentiment.\n",
    "#         How: Regular expressions or string manipulation techniques can be used for this.\n",
    "\n",
    "#     Tokenization:\n",
    "#         Why: Tokenization helps to break down the text into smaller pieces, often into words, which makes it easier for the algorithm to identify patterns.\n",
    "#         How: Libraries like NLTK and spaCy provide tokenization methods.\n",
    "\n",
    "#     Stemming/Lemmatization:\n",
    "#         Why: Different forms of a word often convey the same sentiment (e.g., 'running' and 'ran'). Stemming and Lemmatization convert words to their base or root form.\n",
    "#         How: Again, NLTK and spaCy have methods for these.\n",
    "\n",
    "#     Removal of Stop Words:\n",
    "#         Why: Commonly occurring words (like 'and', 'the', 'is') generally don't contribute to the sentiment and can be removed.\n",
    "#         How: Predefined lists of stop words are available in NLTK and spaCy.\n",
    "\n",
    "#     Feature Extraction:\n",
    "#         Why: Machine learning algorithms require numerical input, and the text needs to be converted into a format like Bag-of-Words or TF-IDF that can be fed into these algorithms.\n",
    "#         How: Scikit-learn provides CountVectorizer for Bag-of-Words and TfidfVectorizer for TF-IDF.\n",
    "\n",
    "#     Handling Class Imbalance:\n",
    "#         Why: Given that your dataset is imbalanced, using techniques to either oversample the minority class or undersample the majority class can make the model more fair.\n",
    "#         How: Libraries like imblearn provide methods like SMOTE for oversampling.\n",
    "\n",
    "\n",
    "\n",
    "#### Data Preprocessing####\n",
    "## Step1: Text Cleaning ## \n",
    "# Remove punctuations\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub('[^\\w\\s]', '', x))\n",
    "\n",
    "## Step 2: Tokenization ##\n",
    "df['tokenized_reviewText'] = df['reviewText'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# ## Step3: Lemmatization ## \n",
    "# from nltk.corpus import wordnet\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# def get_wordnet_pos(tag):\n",
    "#     \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "#     tag = tag[0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# # Function to apply lemmatization to a list of words with POS tagging\n",
    "# def lemmatize_with_pos(words):\n",
    "#     pos_tagged = nltk.pos_tag(words)\n",
    "#     return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged]\n",
    "\n",
    "# # Apply POS tagging and lemmatization\n",
    "# df['lemmatized_reviewText'] = df['tokenized_reviewText'].apply(lemmatize_with_pos)\n",
    "\n",
    "## Step3: Lemmatization ##\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer  # Import the WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to apply lemmatization to a list of words with POS tagging\n",
    "def lemmatize_with_pos(words):\n",
    "    pos_tagged = nltk.pos_tag(words)\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged]\n",
    "\n",
    "# Apply POS tagging and lemmatization\n",
    "df['lemmatized_reviewText'] = df['tokenized_reviewText'].apply(lemmatize_with_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Step4: Removal of Stop Words to reduce dimensionality and complexity ##\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "df['no_stopwords_reviewText'] = df['lemmatized_reviewText'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up the DataFrame \n",
    "df = df[['no_stopwords_reviewText', 'sentiment']]\n",
    "df.rename(columns={'no_stopwords_reviewText': 'reviewText'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step5: Feature Extraction ##\n",
    "# Using Term Frequency-Inverse Document Frequency (TF-IDF) to get numerical representation of the text data# \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['reviewText'] = df['reviewText'].apply(' '.join)\n",
    "X = vectorizer.fit_transform(df['reviewText'])\n",
    "# Now, X is a sparse matrix containing the TF-IDF features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step6: Handling Class Imbalance##\n",
    "# Using Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic data of minority class instances (neutral and negative sentiments) to mitigate class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# Fit on data\n",
    "X_resampled, y_resampled = smote.fit_resample(X, df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7900588939664231\n",
      "Precision: 0.7962748370379209\n",
      "Recall: 0.7900588939664231\n",
      "F1 Score: 0.786500911066001\n"
     ]
    }
   ],
   "source": [
    "## Choosing model and trying it out ##\n",
    "# First lets try using Naive Bayes # \n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Splitting the dataset into Training set and Test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# # Initialize the Multinomial Naive Bayes classifier\n",
    "# nb_classifier = MultinomialNB()\n",
    "\n",
    "# # Fit the model\n",
    "# nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# # Calculating various metrics\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best Score: 0.8400435034601873\n"
     ]
    }
   ],
   "source": [
    "# # Hyperparam tuning: \n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "\n",
    "# # Initialize a Multinomial Naive Bayes classifier\n",
    "# nb_classifier = MultinomialNB()\n",
    "\n",
    "# # Initialize the GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, \n",
    "#                            cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# # Fit data to GridSearch\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Getting the best parameters\n",
    "# best_parameters = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f'Best Parameters: {best_parameters}')\n",
    "# print(f'Best Score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8435766476870067\n",
      "Precision: 0.84341657141689\n",
      "Recall: 0.8435766476870067\n",
      "F1 Score: 0.8429753493290574\n"
     ]
    }
   ],
   "source": [
    "# # retraining the model with found hyperparam: \n",
    "# # Initialize the Multinomial Naive Bayes classifier with the best parameter\n",
    "# nb_classifier = MultinomialNB(alpha=0.001)\n",
    "\n",
    "# nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8318467290633171\n",
      "Precision: 0.8343570154485286\n",
      "Recall: 0.8318467290633171\n",
      "F1 Score: 0.8323641943225352\n"
     ]
    }
   ],
   "source": [
    "# Performance gain of approximately 5% after Fine-Tuning\n",
    "## Logistic regression  ##\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(solver='liblinear')  # 'liblinear' is good for small datasets\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
      "Best Score: 0.9071186652378203\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hyperparameter Finetuning to (maybe) get better results ##\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit data to GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_parameters}')\n",
    "print(f'Best Score: {best_score}')\n",
    "\n",
    "# Retrain the model with best parameters\n",
    "log_reg_best = LogisticRegression(C=best_parameters['C'], penalty=best_parameters['penalty'], solver='liblinear')\n",
    "log_reg_best.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9091908799882701\n",
      "Precision: 0.9105670882128162\n",
      "Recall: 0.9091908799882701\n",
      "F1 Score: 0.908632166225873\n"
     ]
    }
   ],
   "source": [
    "# C of 100 means High variance, low bias\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
