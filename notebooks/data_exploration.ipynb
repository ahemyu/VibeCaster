{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# 'C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Software_5.json'\n",
    "#C:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\data\\Software_5.json(1).gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the datset into DataFrame as described on the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/#subsets\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Industrial_and_Scientific_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77071, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77071 entries, 0 to 77070\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   overall         77071 non-null  float64\n",
      " 1   verified        77071 non-null  bool   \n",
      " 2   reviewTime      77071 non-null  object \n",
      " 3   reviewerID      77071 non-null  object \n",
      " 4   asin            77071 non-null  object \n",
      " 5   style           36037 non-null  object \n",
      " 6   reviewerName    77044 non-null  object \n",
      " 7   reviewText      77060 non-null  object \n",
      " 8   summary         77061 non-null  object \n",
      " 9   unixReviewTime  77071 non-null  int64  \n",
      " 10  vote            9620 non-null   object \n",
      " 11  image           1719 non-null   object \n",
      "dtypes: bool(1), float64(1), int64(1), object(9)\n",
      "memory usage: 7.1+ MB\n",
      "None\n",
      "            overall  unixReviewTime\n",
      "count  77071.000000    7.707100e+04\n",
      "mean       4.524062    1.454857e+09\n",
      "std        0.949668    4.559407e+07\n",
      "min        1.000000    1.051402e+09\n",
      "25%        4.000000    1.427674e+09\n",
      "50%        5.000000    1.459469e+09\n",
      "75%        5.000000    1.486944e+09\n",
      "max        5.000000    1.538093e+09\n"
     ]
    }
   ],
   "source": [
    " # summary statistics\n",
    "print(df.shape) # \n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall               0\n",
      "verified              0\n",
      "reviewTime            0\n",
      "reviewerID            0\n",
      "asin                  0\n",
      "style             41034\n",
      "reviewerName         27\n",
      "reviewText           11\n",
      "summary              10\n",
      "unixReviewTime        0\n",
      "vote              67451\n",
      "image             75352\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete entrys with missing reviewText: \n",
    "df.dropna(subset=['reviewText'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep overall and reviewText\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "columns_to_keep = ['overall', 'reviewText']\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            overall\n",
      "count  77060.000000\n",
      "mean       4.524046\n",
      "std        0.949636\n",
      "min        1.000000\n",
      "25%        4.000000\n",
      "50%        5.000000\n",
      "75%        5.000000\n",
      "max        5.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col sentiment to train the model on\n",
    "\n",
    "def classify_sentiment(overall_score):\n",
    "    if 1 <= overall_score <= 2:\n",
    "        return 1  # Negative\n",
    "    elif overall_score == 3:\n",
    "        return 0  # Neutral\n",
    "    elif 4 <= overall_score <= 5:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Apply the function to the 'overall' column to create the 'sentiment' column\n",
    "df['sentiment'] = df['overall'].apply(classify_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText  sentiment\n",
      "0      5.0  This worked really well for what I used it for...          2\n",
      "1      5.0                   Fast cutting and good adheasive.          2\n",
      "2      5.0  Worked great for my lapping bench.  I would li...          2\n",
      "3      4.0                                      As advertised          2\n",
      "4      5.0  seems like a pretty good value as opposed to b...          2\n",
      "sentiment\n",
      "2    68201\n",
      "0     4442\n",
      "1     4417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "# Count the number of each sentiment\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset is heavily skewed on the positive side of things so might need to use cross-validation to account for it\n",
    "# we only need reviewText and Sentiment to start training our model so lets drop the 'overall' column\n",
    "df.drop(columns=\"overall\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Preprocessing Steps:\n",
    "\n",
    "#     Text Cleaning:\n",
    "#         Why: Raw text often contains punctuations, numbers, and special characters that don't contribute much to the sentiment.\n",
    "#         How: Regular expressions or string manipulation techniques can be used for this.\n",
    "\n",
    "#     Tokenization:\n",
    "#         Why: Tokenization helps to break down the text into smaller pieces, often into words, which makes it easier for the algorithm to identify patterns.\n",
    "#         How: Libraries like NLTK and spaCy provide tokenization methods.\n",
    "\n",
    "#     Stemming/Lemmatization:\n",
    "#         Why: Different forms of a word often convey the same sentiment (e.g., 'running' and 'ran'). Stemming and Lemmatization convert words to their base or root form.\n",
    "#         How: Again, NLTK and spaCy have methods for these.\n",
    "\n",
    "#     Removal of Stop Words:\n",
    "#         Why: Commonly occurring words (like 'and', 'the', 'is') generally don't contribute to the sentiment and can be removed.\n",
    "#         How: Predefined lists of stop words are available in NLTK and spaCy.\n",
    "\n",
    "#     Feature Extraction:\n",
    "#         Why: Machine learning algorithms require numerical input, and the text needs to be converted into a format like Bag-of-Words or TF-IDF that can be fed into these algorithms.\n",
    "#         How: Scikit-learn provides CountVectorizer for Bag-of-Words and TfidfVectorizer for TF-IDF.\n",
    "\n",
    "#     Handling Class Imbalance:\n",
    "#         Why: Given that your dataset is imbalanced, using techniques to either oversample the minority class or undersample the majority class can make the model more fair.\n",
    "#         How: Libraries like imblearn provide methods like SMOTE for oversampling.\n",
    "\n",
    "\n",
    "\n",
    "#### Data Preprocessing####\n",
    "## Step1: Text Cleaning ## \n",
    "# Remove punctuations\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub('[^\\w\\s]', '', x))\n",
    "\n",
    "## Step 2: Tokenization ##\n",
    "df['tokenized_reviewText'] = df['reviewText'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "## Step3: Lemmatization ## \n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to apply lemmatization to a list of words with POS tagging\n",
    "def lemmatize_with_pos(words):\n",
    "    pos_tagged = nltk.pos_tag(words)\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged]\n",
    "\n",
    "# Apply POS tagging and lemmatization\n",
    "df['lemmatized_reviewText'] = df['tokenized_reviewText'].apply(lemmatize_with_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "## Step4: Removal of Stop Words to reduce dimensionality and complexity ##\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "df['no_stopwords_reviewText'] = df['lemmatized_reviewText'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up the DataFrame \n",
    "df = df[['no_stopwords_reviewText', 'sentiment']]\n",
    "df.rename(columns={'no_stopwords_reviewText': 'reviewText'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step5: Feature Extraction ##\n",
    "# Using Term Frequency-Inverse Document Frequency (TF-IDF) to get numerical representation of the text data# \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['reviewText'] = df['reviewText'].apply(' '.join)\n",
    "X = vectorizer.fit_transform(df['reviewText'])\n",
    "# Now, X is a sparse matrix containing the TF-IDF features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step6: Handling Class Imbalance##\n",
    "# Using Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic data of minority class (neutral and negative sentiments)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# Fit on data\n",
    "X_resampled, y_resampled = smote.fit_resample(X, df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
