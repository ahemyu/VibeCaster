{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emovi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# 'C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Software_5.json'\n",
    "#C:\\Users\\emovi\\Desktop\\VibeCaster\\VibeCaster\\data\\Software_5.json(1).gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the datset into DataFrame as described on the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/#subsets\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "# small dataset, ca. 77 000 Entries\n",
    "# df = getDF('C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Industrial_and_Scientific_5.json.gz')\n",
    "# way bigger dataset ca. 500 000 Entries\n",
    "df = getDF('C:\\\\Users\\\\emovi\\\\Desktop\\\\VibeCaster\\\\VibeCaster\\\\data\\\\Video_Games_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497577, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 497577 entries, 0 to 497576\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   overall         497577 non-null  float64\n",
      " 1   verified        497577 non-null  bool   \n",
      " 2   reviewTime      497577 non-null  object \n",
      " 3   reviewerID      497577 non-null  object \n",
      " 4   asin            497577 non-null  object \n",
      " 5   reviewerName    497501 non-null  object \n",
      " 6   reviewText      497419 non-null  object \n",
      " 7   summary         497468 non-null  object \n",
      " 8   unixReviewTime  497577 non-null  int64  \n",
      " 9   vote            107793 non-null  object \n",
      " 10  style           289237 non-null  object \n",
      " 11  image           3634 non-null    object \n",
      "dtypes: bool(1), float64(1), int64(1), object(9)\n",
      "memory usage: 46.0+ MB\n",
      "None\n",
      "             overall  unixReviewTime\n",
      "count  497577.000000    4.975770e+05\n",
      "mean        4.220456    1.367848e+09\n",
      "std         1.185424    1.224113e+08\n",
      "min         1.000000    9.398592e+08\n",
      "25%         4.000000    1.316563e+09\n",
      "50%         5.000000    1.410221e+09\n",
      "75%         5.000000    1.452384e+09\n",
      "max         5.000000    1.538438e+09\n"
     ]
    }
   ],
   "source": [
    " # summary statistics\n",
    "print(df.shape) # \n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall                0\n",
      "verified               0\n",
      "reviewTime             0\n",
      "reviewerID             0\n",
      "asin                   0\n",
      "reviewerName          76\n",
      "reviewText           158\n",
      "summary              109\n",
      "unixReviewTime         0\n",
      "vote              389784\n",
      "style             208340\n",
      "image             493943\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete entrys with missing reviewText: \n",
    "df.dropna(subset=['reviewText', 'overall'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep overall and reviewText\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "columns_to_keep = ['overall', 'reviewText']\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             overall\n",
      "count  497419.000000\n",
      "mean        4.220297\n",
      "std         1.185491\n",
      "min         1.000000\n",
      "25%         4.000000\n",
      "50%         5.000000\n",
      "75%         5.000000\n",
      "max         5.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    393267\n",
      "negative     55012\n",
      "neutral      49140\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create new col sentiment to train the model on\n",
    "df['sentiment'] = df['overall'].apply(lambda x: 'positive' if x > 3 else ('neutral' if x == 3 else 'negative'))\n",
    "# Count the number of each sentiment\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset is heavily skewed on the positive side of things so might need to use cross-validation to account for it\n",
    "# we only need reviewText and Sentiment to start training our model so lets drop the 'overall' column\n",
    "df.drop(columns=\"overall\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emovi\\Desktop\\VibeCaster_dev\\VibeCaster\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Using the BERT tokenizer to preprocess the text.\n",
    "# The function `tokenize_reviews` encodes the text into token IDs, adds special tokens ([CLS], [SEP]), and pads the sequence to a maximum length of 512 tokens.\n",
    "# It also generates an attention mask to differentiate real tokens from [PAD] tokens.\n",
    "# The resulting 'data' column in the DataFrame will contain the token IDs and attention masks for each review.\n",
    " \n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_reviews(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    ")\n",
    "\n",
    "# Apply tokenization to the reviews\n",
    "df['data'] = df['reviewText'].apply(tokenize_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into test and trainingsdata to measure performance later\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                reviewText sentiment  \\\n",
      "0       This game is a bit hard to get the hang of, bu...  positive   \n",
      "1       I played it a while but it was alright. The st...  positive   \n",
      "2                                                ok game.   neutral   \n",
      "3       found the game a bit too complicated, not what...  negative   \n",
      "4       great game, I love it and have played it since...  positive   \n",
      "...                                                   ...       ...   \n",
      "497572                 not OEM but good replacement parts  positive   \n",
      "497573                                        Okay stuff.   neutral   \n",
      "497574  This does add some kids room things that are v...   neutral   \n",
      "497575  I think I originally began playing Bioshock se...  positive   \n",
      "497576  The graphics are terrible, it looks like ps2 g...  negative   \n",
      "\n",
      "                                               data  \n",
      "0       [input_ids, token_type_ids, attention_mask]  \n",
      "1       [input_ids, token_type_ids, attention_mask]  \n",
      "2       [input_ids, token_type_ids, attention_mask]  \n",
      "3       [input_ids, token_type_ids, attention_mask]  \n",
      "4       [input_ids, token_type_ids, attention_mask]  \n",
      "...                                             ...  \n",
      "497572  [input_ids, token_type_ids, attention_mask]  \n",
      "497573  [input_ids, token_type_ids, attention_mask]  \n",
      "497574  [input_ids, token_type_ids, attention_mask]  \n",
      "497575  [input_ids, token_type_ids, attention_mask]  \n",
      "497576  [input_ids, token_type_ids, attention_mask]  \n",
      "\n",
      "[497419 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 42 but got size 7 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emovi\\Desktop\\VibeCaster_dev\\VibeCaster\\Model_Training_BERT.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Creating PyTorch tensors for training and testing data.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Concatenating input_ids and attention_masks for each dataset to form tensors.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Also converting sentiment labels to integer form and forming corresponding tensors.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# This prepares the data in the format required for BERT model training and evaluation.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([item[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m train[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m]], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_attention_masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([item[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m train[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/emovi/Desktop/VibeCaster_dev/VibeCaster/Model_Training_BERT.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap({\u001b[39m'\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m})\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 42 but got size 7 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Creating PyTorch tensors for training and testing data.\n",
    "# Concatenating input_ids and attention_masks for each dataset to form tensors.\n",
    "# Also converting sentiment labels to integer form and forming corresponding tensors.\n",
    "# This prepares the data in the format required for BERT model training and evaluation.\n",
    "\n",
    "import torch\n",
    "\n",
    "train_data = torch.cat([item['input_ids'] for item in train['data']], dim=0)\n",
    "train_attention_masks = torch.cat([item['attention_mask'] for item in train['data']], dim=0)\n",
    "train_labels = torch.tensor(train['sentiment'].map({'positive': 2, 'neutral': 1, 'negative': 0}).values)\n",
    "\n",
    "test_data = torch.cat([item['input_ids'] for item in test['data']], dim=0)\n",
    "test_attention_masks = torch.cat([item['attention_mask'] for item in test['data']], dim=0)\n",
    "test_labels = torch.tensor(test['sentiment'].map({'positive': 2, 'neutral': 1, 'negative': 0}).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
